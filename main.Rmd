---
title: "R Notebook"
output:
  html_document:
    df_print: paged
---
### Reading the data
```{r}
library(readr)
data.2015 <- read_csv("world-happiness/2015.csv")
head(data.2015)
tail(data.2015)
```
### Filtering the data and making it more comfortable for usage
```{r}
require(dplyr)
library(dplyr)

#As soon as we will have data for different years, I create a fuction for filtering and renaming the columns
filter.data <- function(data.set) {
  data.set <- data.set %>%
    rename(
    happ_rank = `Happiness Rank`,
    happ_score = `Happiness Score`,
    GDP = `Economy (GDP per Capita)`,
    family = Family,
    health = `Health (Life Expectancy)`,
    freedom = Freedom,
    trust = `Trust (Government Corruption)`,
  ) %>%
  select(-c(Region, `Standard Error`, Generosity, `Dystopia Residual`, happ_rank))
  return(data.set)
}
```
```{r}
#Updated data
#variables left: country, happ_score, GDP, family, health, freedom, trust
data.2015 <-filter.data(data.2015)
```
### Building a correlation matrix to find the most important variables
```{r}
require(corrplot)
library(corrplot)
require(PerformanceAnalytics)
library("PerformanceAnalytics")
cor_data <- data.2015[, c(2, 3, 4, 5, 6)]
corr_matrix <- cor(cor_data)
#simple correlation matrix
round(corr_matrix, 2)
#visualized
corrplot(corr_matrix, type = "upper", order = "hclust", 
         tl.col = "black", tl.srt = 45)
```
The correlation matrix shows us correlation between all the variables, however we are most interested in variables higly correlated with Happiness Score. 
From this approach we can define family, GDP and health system to be significant but let`s use one more approach.

### Using Random Forest for variable selection
```{r}
require("mlbench")
library(mlbench)
require("caret")
require(caret)
library(caret)
library(randomForest)

model.data <- data.2015[c(2, 3, 4, 5,6, 7)]
#simple linear model
lm.model1 <- lm(happ_score ~., data=model.data)
summary(lm.model1)

#random forest approach
randForest.selection <- randomForest(happ_score ~., data=model.data)
importance(randForest.selection)
```
From the summary of both algorithms, we should definetely exclude trust feature because it isn`t significant.
### Data visualization 1
```{r}
plot(data.2015$GDP, data.2015$happ_score,  xlab= "GDP per capita", ylab = "Happiness Score", main = "2015 GDP vs Happiness Score")
#we see linear relationship between parameters
plot(data.2015$family, data.2015$happ_score, xlab = "Family", ylab= "Happiness Score", main = "2015 Family vs Happiness Score")
#in this case we can spot some outliers, but overall relationship is also linear
plot(data.2015$health, data.2015$happ_score, xlab = "Health", ylab= "Happiness Score", main = "2015 Health vs Happiness Score")

plot(data.2015$freedom, data.2015$happ_score, xlab = "Freedom", ylab= "Happiness Score", main = "2015 Freedom vs Happiness Score")
#not strong correlation

```
### Data visualization 2
```{r}

```

### Simple linear models
```{r}
data.2015.lmodelGDP <- lm(happ_score ~ GDP , data = data.2015)
summary(data.2015.lmodelGDP)
y <- data.2015$happ_score
x <- data.2015$GDP
ggplot(data = data.2015, aes(GDP, happ_score)) + geom_point() + geom_smooth(method = lm, formula = y ~ x)

data.2015.lmodel <- lm(happ_score ~ GDP + family + health, data = data.2015)
summary(data.2015.lmodel)
y <- data.2015$happ_score
x <- data.2015$GDP + data.2015$family + data.2015$health
f <- ggplot(data = data.2015, aes(GDP + family + health, happ_score)) + geom_point() + geom_smooth(method = lm, formula = y ~ x)
#lets test for heteroskedasticity
model_fitted <- data.2015$happ_score - residuals(data.2015.lmodel)
model_fitted
plot(model_fitted, residuals(data.2015.lmodel))
abline(h=0)
```

